<!doctype html>
<html lang="en">
<head>
<title>Teacher-Student Training</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Teacher-Student Training</nobr>
 <!-- <nobr class="widenobr">For DS 4440</nobr> -->
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Distilling the Knowledge in a Neural Network"</h2>
<p>Paper Link: <a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).</p>
<p>Describe the paper and the big question about it that interests you.</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>1) Introduction</h2>

<p>
  For our project, we are exploring the topic of teacher-student training,
  or more formally known as knowledge distillation. It deals with the idea
  of transferring knowledge about certain data from a larger, more complex model
  (the "teacher" model) to a smaller, more efficient "student" model. It is
  a beneficial technique used in machine learning for a variety of reasons.
  
  One of the primary advantages of teacher-student training is model
  compression. By distilling knowledge from a larger, complex "teacher"
  model into a smaller, more lightweight "student" model, we can create
  models that are faster and more resource-efficient for deployment on
  devices with limited computational resources, such as mobile phones or
  IoT devices.

  In addition, experiments have shown that student models trained using
  knowledge distillation often achieve better performance in terms of
  its accuracy and efficiency as compared to models trained directly on the
  original dataset. This is in part due to the improved generalization
  that teacher-student training encourages. Instead of learning from the
  hard targets (where the probability of each output is either a 1 or a 0),
  the student model learns from soft targets (a more realistic target
  distribution over all classes where instead of outputs having a
  probability of 0% or 100%, they can have probabilities of 20% and 80%,
  for example.) In teacher-student training, the dataset provides hard
  targets (a single target label) and the teacher provides soft targets
  (a distribution over all labels).

  We are using a pre-trained model as our teacher model for this project.
  The model was pre-trained on the well-known image classification CIFAR-10
  data set. 
</p>

<p>
  <img alt="cifar10 example images" class="img-fluid" src="images/cifar_pics.png" style="width:300;height:100px;">
</p>


<h2>2) Paper Summary</h2>

<p>
  <img alt="softmax with temperature" class="img-fluid" src="images/softmax_with_temp.png" style="width:300;height:100px;">
</p>


<h2>3) Implementation</h2>

<p>
  We had access to the following large pre-trained models from the <a href="https://github.com/huyvnphan/PyTorch_CIFAR10">PyTorch_CIFAR10 repo</a>:
</p>

<img alt="table of pre-trained models" class="img-fluid" src="images/pytorch_cifar10.png" style="width:320;height:400px;">

<p>
  <br>
  In our initial test we used vgg13_bn (94.22% val accuracy) as the teacher model, which has the following architecture:
</p>

<img alt="vgg13_bn architecture" class="img-fluid" src="images/teacher.png" style="width:400;height:500px;">

<p>
  <br>
  We set up a very simple student model as a proof of concept. It used the following architecture:
</p>

<img alt="student model architecture" class="img-fluid" src="images/student.png" style="width:500;height:400px;">

<p>
  <br>
  Our training loop was set up like so:
</p>

<img alt="training code" class="img-fluid" src="images/train.png" style="width:500;height:400px;">

<p>
  <br>
  It is important to note what is being compared in the loss function. The student model's output logits are fed through a softmax
  function with an increased temperature. This is then compared to a weighted average of the teacher model's output (also fed through
  a softmax with increased temperature) and the ground-truth one-hot label.
</p>


<h2>4) Results</h2>

<p>
  The very first model we trained, small, and trained for only two epochs, yielded the following accuracy by class:
</p>

<img alt="training code" class="img-fluid" src="images/accuracy.png">

<p>
  <br>
  These are some interesting results, but we definitely have some work to do still!
</p>


<h2>5) Conclusion</h2>

<p>
  Conclusion paragraph
</p>


<h3>References</h3>

<p><a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).
</p>

<h2>Team Members</h2>
                                                   
<p>Tijana Cosic | cosic.t@northeastern.edu, 
   George Bikhazi | bikhazi.g@northeastern.edu
</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440: Practical Neural Networks</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
