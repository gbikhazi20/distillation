<!doctype html>
<html lang="en">
<head>
<title>Teacher-Student Training</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Teacher-Student Training</nobr>
 <!-- <nobr class="widenobr">For DS 4440</nobr> -->
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Distilling the Knowledge in a Neural Network"</h2>
<p>Paper Link: <a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).</p>
<h4>by George Bikhazi and Tijana Cosic</h4>
</div>
</div>
<div class="row">
<div class="col">

<h2>1) Introduction</h2>

<p>
  For our project, we are exploring the topic of teacher-student training,
  or more formally known as knowledge distillation. It deals with the idea
  of transferring knowledge about certain data from a larger, more complex model
  (the "teacher" model) to a smaller, more efficient "student" model. It is
  a beneficial technique used in machine learning for a variety of reasons.
  
  One of the primary advantages of teacher-student training is model
  compression. By distilling knowledge from a larger, complex "teacher"
  model into a smaller, more lightweight "student" model, we can create
  models that are faster and more resource-efficient for deployment on
  devices with limited computational resources, such as mobile phones or
  IoT devices.

  In addition, experiments have shown that student models trained using
  knowledge distillation often achieve better performance in terms of
  its accuracy and efficiency as compared to models trained directly on the
  original dataset. This is in part due to the improved generalization
  that teacher-student training encourages. Instead of learning from the
  hard targets (where the probability of each output is either a 1 or a 0),
  the student model learns from soft targets (a more realistic target
  distribution over all classes where instead of outputs having a
  probability of 0% or 100%, they can have probabilities of 20% and 80%,
  for example.) In teacher-student training, the dataset provides hard
  targets (a single target label) and the teacher provides soft targets
  (a distribution over all labels).

  We are using a pre-trained model as our teacher model for this project.
  The model was pre-trained on the well-known image classification CIFAR-10
  data set. 
</p>

<p>
  <img alt="cifar10 example images" class="img-fluid" src="images/cifar_pics.png" style="width:300;height:100px;">
</p>


<h2>2) Paper Summary</h2>
<p>The paper highlights the way in which distillation works. Neural networks
   will typically produce class probabilities by using a “softmax” output 
   layer that converts the logit, z_i, computed for each class into a
   probability, q_i, by comparing z_i with the other logits. The formula is
   as follows:
</p>
<p>
  <img alt="softmax with temperature" class="img-fluid" src="images/softmax_with_temp.png" style="width:300;height:100px;">
</p>
<p>In the simplest form of distillation, knowledge is transferred to the
  distilled model by training it on a transfer set and using a soft target
  distribution for each case in the transfer set that is produced by using
  the larger, complex model with a higher temperature in its softmax.
  The same high temperature is used when training the distilled model,
  but after it has been trained it uses a temperature (T) of 1. However,
  using a higher value for T produces a softer probability distribution
  over classes.
</p>

<h2>3) Implementation</h2>

<p>
  We had access to the following large pre-trained models from the <a href="https://github.com/huyvnphan/PyTorch_CIFAR10">PyTorch_CIFAR10 repo</a>:
</p>

<img alt="table of pre-trained models" class="img-fluid" src="images/pytorch_cifar10.png" style="width:320;height:400px;">

<p>
  <br>
  In our initial test we used vgg13_bn (94.22% val accuracy) as the teacher model, which has the following architecture:
</p>

<img alt="vgg13_bn architecture" class="img-fluid" src="images/teacher.png" style="width:400;height:500px;">

<p>
  <br>
  We set up a very simple student model as a proof of concept. It used the following architecture:
</p>

<img alt="student model architecture" class="img-fluid" src="images/student.png" style="width:500;height:400px;">

<p>
  <br>
  Our training loop was set up like so:
</p>

<img alt="training code" class="img-fluid" src="images/train.png" style="width:500;height:400px;">

<p>
  <br>
  It is important to note what is being compared in the loss function. The student model's output logits are fed through a softmax
  function with an increased temperature. This is then compared to a weighted average of the teacher model's output (also fed through
  a softmax with increased temperature) and the ground-truth one-hot label.
</p>


<h2>4) Results</h2>

<p>
  The very first model we trained, small, and trained for only two epochs, yielded the following accuracy by class:
</p>

<img alt="training code" class="img-fluid" src="images/accuracy.png">

<p>
  <br>
  We trained a few more student models with different architectures as well. In
  addition to experimenting with various student models, we also wanted to see the
  difference in results by using either just the hard targets, soft targets, and then
  a combination of both to train the student model. Below are some of our results:
</p>

<img alt="acc by model" class="img-fluid" src="images/acc_by_model.PNG">
<p>
  In this image, we see that the student model that was trained on just the hard
  targets performed worse than the models trained on the soft targets and the mix of
  soft and hard targets. This supports the paper's argument that using probabilities
  of all classes for predicting a class is advantageous and provides improved accuracy.   
</p>

<p>
  <br>
  These are our accuracies by class for the soft targets student model and the mix
  of both soft and hard targets model:   
</p>

<img alt="acc by class st" class="img-fluid" src="images/acc_by_class_st.PNG">
<img alt="acc by class mix" class="img-fluid" src="images/acc_by_class_mix.PNG">

<p>
  From these two graphs, we can see that about half of the classes were more often 
  classified correctly in the soft targets model, and the other half of the classes were 
  more often correctly classified in the model trained on the mix of soft and hard 
  targets.  
</p>

<p>
  <br>
  Lastly, we created confusion matrices to additionally show what classes the models
  were getting correct/incorrect the most.
</p>

<img alt="conf matrix st" class="img-fluid" src="images/conf_matrix_st.PNG">
<img alt="conf matrix mix" class="img-fluid" src="images/conf_matrix_mix.PNG">

<p>
  We were intrigued to find that part of the mistakes the models were making was that
  they were specifically confusing modes of transportation, including a car, a plane,
  a ship, and a truck. In addition, they were confusing classes in the middle of the 
  matrix, such as birds, dogs, cats, and deers. 
</p>

<h2>5) Conclusion</h2>

<p>
  Our project was mainly experimenting with different ways of training a student model 
  from a pre-trained teacher model in order to determine how advantageous and efficient 
  it is. We ran multiple student models with various architectures in order to see if 
  the number of convolutional layers affects the model's performance. We also trained 
  on the soft targets only, the hard targets only, and on the mix both. In the end, we 
  discovered that using soft targets does, in fact, improve the model's performance.
  For future work, we would like to use different pre-trained models as the teacher 
  model to see if this will make a difference when training student models.
</p>

<h3>References</h3>

<p><a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).
</p>

<h2>Team Members</h2>
                                                   
<p>Tijana Cosic | cosic.t@northeastern.edu, 
   George Bikhazi | bikhazi.g@northeastern.edu
</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440: Practical Neural Networks</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
