<!doctype html>
<html lang="en">
<head>
<title>Teacher-Student Training</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Teacher-Student Training</nobr>
 <!-- <nobr class="widenobr">For DS 4440</nobr> -->
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "Distilling the Knowledge in a Neural Network"</h2>
<p>Paper Link: <a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).</p>
<p>Describe the paper and the big question about it that interests you.</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>1) Introduction</h2>

<p>
  For our project, we are exploring the topic of teacher-student training,
  or more formally known as knowledge distillation. It deals with the idea
  of transferring knowledge about certain data from a larger, more complex model
  (the "teacher" model) to a smaller, more efficient "student" model. It is
  a beneficial technique used in machine learning for a variety of reasons.
  
  One of the primary advantages of teacher-student training is model
  compression. By distilling knowledge from a larger, complex "teacher"
  model into a smaller, more lightweight "student" model, we can create
  models that are faster and more resource-efficient for deployment on
  devices with limited computational resources, such as mobile phones or
  IoT devices.

  In addition, experiments have shown that student models trained using
  knowledge distillation often achieve better performance in terms of
  its accuracy and efficiency as compared to models trained directly on the
  original dataset. This is in part due to the improved generalization
  that teacher-student training encourages. Instead of learning from the
  hard targets (where the probability of each output is either a 1 or a 0),
  the student model learns from soft targets (a more realistic target
  distribution over all classes where instead of outputs having a
  probability of 0% or 100%, they can have probabilities of 20% and 80%,
  for example.) In teacher-student training, the dataset provides hard
  targets (a single target label) and the teacher provides soft targets
  (a distribution over all labels).

  We are using a pre-trained model as our teacher model for this project.
  The model was pre-trained on the well-known image classification CIFAR-10
  data set. 
</p>

<p>
  <img alt="tiny picture of a car" class="img-fluid" src="images/car.png" style="width:150;height:150px;">>
</p>


<h2>2) Paper Summary</h2>

<p>
  Paper summary
</p>


<h2>3) Implementation</h2>

<p>
  Implementation details
</p>


<h2>4) Results</h2>

<p>
  Results discussion
</p>


<h2>5) Conclusion</h2>

<p>
  Conclusion paragraph
</p>


<h3>References</h3>

<p><a name="bottou-1990">[1]</a> <a href="https://arxiv.org/pdf/1503.02531.pdf"
  >Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
  <em>Distilling the Knowledge in a Neural Network</em></a>
  NIPS Deep Learning and Representation Learning Workshop (2015).
</p>

<h2>Team Members</h2>
                                                   
<p>Tijana Cosic | cosic.t@northeastern.edu, 
   George Bikhazi | bikhazi.g@northeastern.edu
</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://ds4440.baulab.info/">About DS 4440: Practical Neural Networks</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
